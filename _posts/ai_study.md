# 人工智能基础

## 人工智能是一把双刃剑
### 推动经济社会发展
### 对人类迁移默化的改造
#### 超人工智能毁灭人类
#### 人沦为机器的附庸
人工智能本质是一种劳动工具，但当劳动工具强大到是人类成了多余的角色，有降格为“亚人工智能”的危险。

### 人类当如何应对
#### 专精于依赖创造力的领域
#### 掌握核心技术

### 人工智能的发展
从早期遵循符号主义到连接主义（以工程技术手段模拟人脑神经系统的结构和功能来模拟人类智能）

## 人工智能的知识体系
### 数学基础
蕴含处理智能问题的基本思想和方法
#### 线性代数
#### 概率论
#### 最优化方法
### 机器学习
从数据中学习算法，解决实际应用问题
#### 线性回归
#### 决策树
#### 支持向量机
#### 聚类
### 人工神经网络
机器学习分支，认知科学引入机器学习，以模拟生物神经系统对真实世界的交互反映
#### 多层神经网络
#### 前馈和反向传播
#### 自组织神经网络
### 深度学习
多个中间层的神经网络，数据爆炸和计算力的飙升
#### 深度前馈网络
#### 深度学习中的正则化
#### 自动编码
### 神经网络实例
神经网络在不同场景下的应用
#### 卷积神经网络
#### 递归神经网络
#### 深度信念网络
### 其他
深度学习局限性的有益补充
#### 马尔可夫随机场
#### 迁移学习
#### 集群智能

## 人工只能的应用
### 计算机视觉
### 语音识别
### 对话系统

#线性代数
人工智能是摩天大楼，线性代数就是砖头，现代数学的基础

## 线性代数的意义
万事万物都可以被抽象成某些特征的组合，并在由预置规则定义的框架之下以静态和动态的方式加以观察。

计算机智能处理离散取值的二进制信息，因此来自模拟世界的信号必须在定义域和值域上同时进行数字化，才能被计算机存储和处理。从这个角度看，线性代数是用虚拟数字世界表示真实物理世界的工具。
## 线性代数的基本概念
### 集合
由某些特定对象汇总而成的集体。
集合中的元素通常会具有某些共性，因而可以用这些共性来表示。
｛苹果、橘子、梨子｝、｛牛、马、羊｝
在数学处理的范围，集合的元素需要进一步抽象---用数字和符号来表示。
### 标量
由单独的数a构成的元素称为标量。一个标量a可以为整数、实数或复数。
### 向量
多个标量a1……an按一定顺序组成一个序列
向量实质是n维线性空间中的静止点
每个向量都由若干标量构成
### 矩阵
将向量的所有标量都替换成相同规格的向量
#### 特征值和特征向量
### 张量
将矩阵中的每个标量替换为向量，张量就是高阶矩阵
三阶魔方是一个3*3*3的张量，3*3的矩阵是张量的一个切片。

### 关系
计算机存储中，标量占据的是零维数组，向量占用一维数组，例如语音信号；矩阵占据二维数组，例如灰度图像；张量占据三维乃至更高的维度的数组，例如RGB图像和视频

### 向量描述
#### 范数
范数是对单个向量大小的度量，描述的是向量自身的性质，其作用是将向量映射为一个非负的数值。
##### 通用范数定义
L1：向量所有元素绝对值的和
L2：向量的长度
L3：向量中最大元素的取值
#### 内积
范数计算是单个向量的尺度，内积计算是两个向量之间的关系。
内积能表示两个向量之间的相对位置，即向量之间的夹角。
内积为0，即<x,y>=0.在二维空间上，代表两个向量夹角为90度，即相互垂直。
而高维空间上，这种关系被称为正交。如果两个向量正交则说明他们线性无关。
向量的意义不仅是某些数字的组合，更可能是某些对象、行为特征。范式和内积能够处理这些表示特征的数学模型，进而提取出原始对象或原始行为中的隐含关系。
#### 线性空间
如果有一个集合，它的元素都是具有相同维数的向量（有限个或无限个），并且定义了加法和数乘等结构化运算，这样的集合就被称为线性空间。
##### 内积空间
定义内积运算的线性空间称之为内积空间。
在线性空间中，任意一个向量代表的都是n维空间的一个点，反之，空间中的任意点也都可以唯一地用一个向量表示。
##### 参考系
在线性空间上点和向量的相互映射中，一个关键的问题是参考系的选取。
例如：经纬度和海拔高度构成的三维向量（x,y,h）就对应了三维物理空间中的一个点。

#### 正交基
对于复杂空间则需要正交基这一概念。
在内积空间中，一组两两正交的向量构成这个空间的正交基。
#### 标准正交基
正交基中基向量的L2范数都是单位长度1，这组正交基就是标准正交基
##### 正交基的作用
给内积空间定义出经纬度。一旦描述内积空间的正交基确定，向量和点之间的对应关系也就随之确定。
##### 注意
内积空间的正交基并不唯一。对二维空间来说，平面直角坐标系和极坐标系就对应两组不同的正交基，也代表两种实用描述方式。

#### 线性变换
线性空间的一个重要特征是能够承载变化。当标准正交基确定后，空间中的点就可以用向量表示，位置变化会导致向量改变。点的变化对应向量的线性变换，而描述对象变化抑或向量变化的数学语言，正是矩阵。

##### 变换方式
###### 点本身的变化
代表变化的矩阵乘以代表对象的向量。
###### 参考系的变化
保持点不变，换一种观察角度，得到的结果也将不同。
“横看成岭侧成峰，远近高低各不同”
对于矩阵和向量相乘，就存在不同的解读方式：Ax=y。
可以理解为向量x经过矩阵A所描述的变换，变成了向量y；也可理解为一个对象在坐标系A的度量下得到的结果为向量x，在坐标系单位矩阵I，主对角线元素为1，其余元素为0）的度量下得到的结果为向量y。
这表明矩阵不仅能够描述变化，也可以描述参考系本身。
对坐标施加变换的方法，就是让表示原始坐标系的矩阵与表示变换的矩阵相乘。

#####矩阵的特征值和特征向量
对于给定矩阵A，假设其特征值为lambda，特征向量为x，则它们的关系为：Ax=lambdax
矩阵的特征向量是一种只有尺度变化没有方向变化的特殊向量，也就是只有伸缩效果没有旋转效果。特征向量的尺度变化系数就是特征值。

矩阵特征值和特征向量的动态意义在于表示了变化的速度和方向。
矩阵变化看作奔跑的人，特征值代表奔跑的速度，特征向量代表奔跑的方向
矩阵可以以不同的速度在不同的方向上奔跑，叠加在一起才是矩阵的效果

##### 特征值分解
求解给定矩阵的特征值和特征向量的过程叫做特征值分解。
只有n维方阵才能进行特征值分解。
特征值算法推广到所有矩阵上，就是更加通用的奇异值分解。















